{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "Co-js5zHMb-k"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "import torch\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms as transforms\n",
        "import torch.utils.data as dataloader\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#transformation of PIL data into tensor format\n",
        "transformation_operation=transforms.Compose([transforms.ToTensor()])"
      ],
      "metadata": {
        "id": "mrCNLRZMNOdP"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset=torchvision.datasets.MNIST(root='./data', train =True, download=True, transform=transformation_operation)"
      ],
      "metadata": {
        "id": "B85Nwu53MuTX"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset=torchvision.datasets.MNIST(root='./data', train =False, download=True, transform=transformation_operation)"
      ],
      "metadata": {
        "id": "9sr_pw_TNmXq"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=64\n",
        "num_classes=10\n",
        "img_size=28\n",
        "patch_size=7\n",
        "patch_number = (img_size//patch_size) * (img_size//patch_size)\n",
        "attention_heads= 4\n",
        "embed_dim = 20\n",
        "transformer_blocks = 4\n",
        "mlp_nodes= 64\n",
        "num_channels = 1 # black and white image\n",
        "learning_rate= 0.001\n",
        "epochs=5"
      ],
      "metadata": {
        "id": "fxD3iUcmOmt_"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# using dataloader to prep data for nueral n/w\n",
        "\n",
        "train_data= dataloader.DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
        "val_data=dataloader.DataLoader(train_dataset, shuffle=True, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "25a3e_3eNlQR"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class for PatchEmbedding - part 1\n",
        " # inherits from nn.Module\n",
        "class PatchEmbedding(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    # stride size is same as patch size as they are non overlapping\n",
        "    # kernel size is also same as patch size\n",
        "    self.patch_embed = nn.Conv2d(num_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "\n",
        "  def forward(self,x): # x is the input dataset -> variable generated from training data\n",
        "    x=self.patch_embed(x)\n",
        "    x=x.flatten(2)\n",
        "    x=x.transpose(1,2)\n",
        "    return x\n",
        "\n"
      ],
      "metadata": {
        "id": "obuZZnF-RAYn"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to print dims\n",
        "\n",
        "images, labels = next(iter(train_data))\n",
        "print(images.shape)\n",
        "\n",
        "#torch.Size([64, 1, 28, 28])\n",
        "            #batch size,no of channels, size of image\n",
        "\n",
        "patch_embed = nn.Conv2d(num_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "print(\"This is the shape of input image tensor\", images.shape)\n",
        "print(\"This is the shape of output patch embed image tensor\", patch_embed(images).shape)\n",
        "\n",
        "'''\n",
        "This is the shape of input image tensor torch.Size([64, 1, 28, 28])\n",
        "This is the shape of output patch embed image tensor torch.Size([64, 20, 4, 4])\n",
        "\n",
        "'''\n",
        "embedded_image= patch_embed(images)\n",
        "print(\"This is the shape of flattened image tensor\",embedded_image.flatten(2).shape)\n",
        "print(\"This is the shape of  transposed image tensor\",embedded_image.flatten(2).transpose(1,2).shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awI9ZlmdT4Qp",
        "outputId": "bd7f21a4-c981-4a0f-d902-10bece0ad49e"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 1, 28, 28])\n",
            "This is the shape of input image tensor torch.Size([64, 1, 28, 28])\n",
            "This is the shape of output patch embed image tensor torch.Size([64, 20, 4, 4])\n",
            "This is the shape of flattened image tensor torch.Size([64, 20, 16])\n",
            "This is the shape of  transposed image tensor torch.Size([64, 16, 20])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# class for Transformer encoder - part 2\n",
        "# Layer Norm\n",
        "# Multi-Head Attention\n",
        "# Layer Norm\n",
        "# Residuals\n",
        "# MLP - activation function\n",
        "class TransformerEncoder(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.layer_norm1=nn.LayerNorm(embed_dim)\n",
        "    self.multi_head_attention=nn.MultiheadAttention(embed_dim,attention_heads, batch_first=True)\n",
        "    self.layer_norm2=nn.LayerNorm(embed_dim)\n",
        "    self.mlp=nn.Sequential(\n",
        "        nn.Linear(embed_dim, mlp_nodes),\n",
        "        nn.GELU(),\n",
        "        nn.Linear(mlp_nodes, embed_dim),\n",
        "        #nn.GELU(),\n",
        "        #nn.Linear(embed_dim)\n",
        "    )\n",
        "  def forward(self,x):\n",
        "    residual1=x\n",
        "    x=self.layer_norm1(x)\n",
        "    x=self.multi_head_attention(x, x, x)[0] # key, query and value\n",
        "    x=x+residual1\n",
        "    residual2=x\n",
        "    x=self.layer_norm2(x)\n",
        "    x=self.mlp(x)\n",
        "    x=x+residual2\n",
        "    return x\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "n9uzkZ_5Q_c6"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class for MLP head for classification encoder - part 3\n",
        "class MLP_Head(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    # to prev overfitting\n",
        "    self.layer_norm1 = nn.LayerNorm(embed_dim)\n",
        "    self.mlphead=nn.Sequential(\n",
        "        #nn.Linear(embed_dim),\n",
        "        nn.Linear(embed_dim,num_classes)\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    # we need only vector associated with CLS for classificatio\n",
        "    #x = x[:,0]\n",
        "    x=self.layer_norm1(x)\n",
        "    x=self.mlphead(x)\n",
        "\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "In14YJ2LRdue"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VisionTransformer(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.patch_embedding = PatchEmbedding()\n",
        "    self.cls_token = nn.Parameter(torch.randn(1,1,embed_dim))\n",
        "    self.position_embedding = nn.Parameter(torch.randn(1,patch_number+1, embed_dim))\n",
        "    # for n number of transformer blocks\n",
        "    self.transformer_blocks= nn.Sequential(*[TransformerEncoder() for _ in range(transformer_blocks)])\n",
        "    self.mlp_head = MLP_Head()\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.patch_embedding(x)\n",
        "    B = x.size(0) # cannot be hardcoded as last batch may have 16 images or any number\n",
        "    cls_tokens = self.cls_token.expand(B,-1,-1)\n",
        "    x=torch.cat((cls_tokens,x),1)\n",
        "    x= x+self.position_embedding\n",
        "    x = x[:,0]\n",
        "    x = self.mlp_head(x)\n",
        "\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "xjwWpPHRUD92"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# optimizer\n",
        "# cross entropy loss\n",
        "\n",
        "# device\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = VisionTransformer().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()\n"
      ],
      "metadata": {
        "id": "f4RTE_DnhjhJ"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(5):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct_epoch = 0\n",
        "    total_epoch = 0\n",
        "    print(f\"\\nEpoch {epoch+1}\")\n",
        "\n",
        "    for batch_idx, (images, labels) in enumerate(train_data):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss+=loss.item()\n",
        "        preds = outputs.argmax(dim=1)\n",
        "\n",
        "        correct = (preds == labels).sum().item()\n",
        "        accuracy = 100.0 * correct / labels.size(0)\n",
        "\n",
        "        correct_epoch += correct\n",
        "        total_epoch += labels.size(0)\n",
        "\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f\"  Batch {batch_idx+1:3d}: Loss = {loss.item():.4f}, Accuracy = {accuracy:.2f}%\")\n",
        "\n",
        "    epoch_acc = 100.0 * correct_epoch / total_epoch\n",
        "    print(f\"==> Epoch {epoch+1} Summary: Total Loss = {total_loss:.4f}, Accuracy = {epoch_acc:.2f}%\")"
      ],
      "metadata": {
        "id": "0TEGUDA3iRhO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52425ac4-79b8-440e-a281-feb501c012ff"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1\n",
            "  Batch   1: Loss = 2.3044, Accuracy = 15.62%\n",
            "  Batch 101: Loss = 2.3000, Accuracy = 12.50%\n",
            "  Batch 201: Loss = 2.3059, Accuracy = 7.81%\n",
            "  Batch 301: Loss = 2.3026, Accuracy = 3.12%\n",
            "  Batch 401: Loss = 2.3029, Accuracy = 10.94%\n",
            "  Batch 501: Loss = 2.3228, Accuracy = 7.81%\n",
            "  Batch 601: Loss = 2.3054, Accuracy = 12.50%\n",
            "  Batch 701: Loss = 2.3005, Accuracy = 6.25%\n",
            "  Batch 801: Loss = 2.3182, Accuracy = 9.38%\n",
            "  Batch 901: Loss = 2.3055, Accuracy = 9.38%\n",
            "==> Epoch 1 Summary: Total Loss = 2159.9498, Accuracy = 10.91%\n",
            "\n",
            "Epoch 2\n",
            "  Batch   1: Loss = 2.3088, Accuracy = 3.12%\n",
            "  Batch 101: Loss = 2.3024, Accuracy = 10.94%\n",
            "  Batch 201: Loss = 2.2938, Accuracy = 14.06%\n",
            "  Batch 301: Loss = 2.3054, Accuracy = 7.81%\n",
            "  Batch 401: Loss = 2.3095, Accuracy = 9.38%\n",
            "  Batch 501: Loss = 2.2920, Accuracy = 21.88%\n",
            "  Batch 601: Loss = 2.2893, Accuracy = 12.50%\n",
            "  Batch 701: Loss = 2.3021, Accuracy = 7.81%\n",
            "  Batch 801: Loss = 2.3100, Accuracy = 3.12%\n",
            "  Batch 901: Loss = 2.3031, Accuracy = 7.81%\n",
            "==> Epoch 2 Summary: Total Loss = 2159.9039, Accuracy = 10.84%\n",
            "\n",
            "Epoch 3\n",
            "  Batch   1: Loss = 2.3031, Accuracy = 3.12%\n",
            "  Batch 101: Loss = 2.3138, Accuracy = 12.50%\n",
            "  Batch 201: Loss = 2.2991, Accuracy = 12.50%\n",
            "  Batch 301: Loss = 2.2781, Accuracy = 18.75%\n",
            "  Batch 401: Loss = 2.3091, Accuracy = 7.81%\n",
            "  Batch 501: Loss = 2.3110, Accuracy = 15.62%\n",
            "  Batch 601: Loss = 2.3096, Accuracy = 6.25%\n",
            "  Batch 701: Loss = 2.3002, Accuracy = 9.38%\n",
            "  Batch 801: Loss = 2.3060, Accuracy = 10.94%\n",
            "  Batch 901: Loss = 2.2921, Accuracy = 10.94%\n",
            "==> Epoch 3 Summary: Total Loss = 2159.8094, Accuracy = 11.05%\n",
            "\n",
            "Epoch 4\n",
            "  Batch   1: Loss = 2.3018, Accuracy = 6.25%\n",
            "  Batch 101: Loss = 2.2975, Accuracy = 9.38%\n",
            "  Batch 201: Loss = 2.2968, Accuracy = 10.94%\n",
            "  Batch 301: Loss = 2.3011, Accuracy = 14.06%\n",
            "  Batch 401: Loss = 2.3080, Accuracy = 9.38%\n",
            "  Batch 501: Loss = 2.2943, Accuracy = 10.94%\n",
            "  Batch 601: Loss = 2.2984, Accuracy = 10.94%\n",
            "  Batch 701: Loss = 2.3009, Accuracy = 9.38%\n",
            "  Batch 801: Loss = 2.3210, Accuracy = 6.25%\n",
            "  Batch 901: Loss = 2.3085, Accuracy = 12.50%\n",
            "==> Epoch 4 Summary: Total Loss = 2159.8494, Accuracy = 10.70%\n",
            "\n",
            "Epoch 5\n",
            "  Batch   1: Loss = 2.3185, Accuracy = 6.25%\n",
            "  Batch 101: Loss = 2.2985, Accuracy = 14.06%\n",
            "  Batch 201: Loss = 2.2973, Accuracy = 10.94%\n",
            "  Batch 301: Loss = 2.3001, Accuracy = 12.50%\n",
            "  Batch 401: Loss = 2.2999, Accuracy = 14.06%\n",
            "  Batch 501: Loss = 2.2969, Accuracy = 18.75%\n",
            "  Batch 601: Loss = 2.3047, Accuracy = 12.50%\n",
            "  Batch 701: Loss = 2.2980, Accuracy = 9.38%\n",
            "  Batch 801: Loss = 2.2990, Accuracy = 10.94%\n",
            "  Batch 901: Loss = 2.3050, Accuracy = 9.38%\n",
            "==> Epoch 5 Summary: Total Loss = 2159.9019, Accuracy = 10.90%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JHpxYK-ZxO_v"
      },
      "execution_count": 60,
      "outputs": []
    }
  ]
}